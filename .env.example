# ScholarPipe Configuration

# ============================================
# LLM API Keys
# ============================================

# Zhipu AI API Key (for zhipu provider)
# Get your key from: https://open.bigmodel.cn/
ZHIPU_API_KEY=your_zhipu_api_key_here

# SiliconFlow API Key (for siliconflow provider)
# Get your key from: https://siliconflow.cn/
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# MiMo API Key (for mimo provider)
# Get your key from: https://xiaomimimo.com/
MIMO_API_KEY=your_mimo_api_key_here

# ============================================
# Semantic Model (Step 4) Configuration
# ============================================

# Preset: dev | prod | api | custom (also configurable in config/settings.yaml)
#   dev  - all-MiniLM-L6-v2 on CPU (~80MB, fast for testing)
#   prod - BAAI/bge-m3 on GPU (~2GB, best quality)
#   api  - DashScope cloud API (no local GPU needed)
# SEMANTIC_PRESET=dev

# DashScope API Key (required for api preset)
# Get your key from: https://dashscope.console.aliyun.com/
DASHSCOPE_API_KEY=your_dashscope_api_key_here

# Override embedding model (optional, overrides preset)
# HYBRID_EMBED_MODEL=BAAI/bge-m3

# Batch size for embedding (default: 16)
# Increase if you have more VRAM, decrease if OOM
# HYBRID_EMBED_BATCH=16

# Embedding instruction template (only for models that need it)
# HYBRID_EMBED_INSTRUCTION=Represent passage for retrieval: {}

# ============================================
# HuggingFace Cache (Optional)
# ============================================

# Custom cache directory for models (default: ~/.cache/huggingface)
# HF_HOME=/path/to/custom/cache

# ============================================
# WeChat Official Account (公众号推送)
# ============================================

# 公众号 AppID 和 AppSecret
# 获取位置: 公众号后台 → 设置与开发 → 基本配置
WX_APP_ID=your_wechat_app_id
WX_APP_SECRET=your_wechat_app_secret
