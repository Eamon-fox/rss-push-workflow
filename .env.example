# ScholarPipe Configuration

# ============================================
# LLM API Keys
# ============================================

# Zhipu AI API Key (for zhipu provider)
# Get your key from: https://open.bigmodel.cn/
ZHIPU_API_KEY=your_zhipu_api_key_here

# SiliconFlow API Key (for siliconflow provider)
# Get your key from: https://siliconflow.cn/
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# ============================================
# Hybrid Filter (Step 4) - GPU Configuration
# ============================================

# Embedding model (default: BAAI/bge-m3)
# HYBRID_EMBED_MODEL=BAAI/bge-m3

# Batch size for embedding (default: 16)
# Increase if you have more VRAM, decrease if OOM
# HYBRID_EMBED_BATCH=16

# Embedding instruction template (default: "Represent passage for retrieval: {}")
# HYBRID_EMBED_INSTRUCTION=Represent passage for retrieval: {}

# ============================================
# HuggingFace Cache (Optional)
# ============================================

# Custom cache directory for models (default: ~/.cache/huggingface)
# HF_HOME=/path/to/custom/cache
